\documentclass[11pt,a4paper]{colorart}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{latexsym}
\usepackage{amsfonts,amssymb}
\usepackage{psfrag}
\usepackage{ulem}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{physics}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{tikz-cd}
\usepackage{import, pdfpages, transparent}
\usepackage{caption, subcaption}
\usepackage[section]{placeins}
\usepackage{hyperref}

\def\nn{\nonumber} 
\def\pa{{\partial}}
\def\f{\frac}
\def\l{\left}
\def\r{\right}
\def\d{{\rm d}}
\def\es{\emptyset}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}}
\def\Q{\mathbb{Q}}
\def\e{\epsilon}
\def\a{\alpha}
\def\b{\beta}
\def\g{\gamma}
\def\p{\phi}
\def\Ker{\text{Ker}}
\def\Con{\text{Con}}
\def\L{\mathcal{L}}

\title{\huge Linear Algebra}
\author{Incalculas}
\date{Updated on: \today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Vector Space}

\subsection{What is a Vector Space?}

Vector space is a set with a binary operation
	\[ +: V \times V \rightarrow V \]

Key properties of a vector space:

\begin{itemize}
	\item \textit{i) Commutativity}
			\[A + B = B + A \]
	\item \textit{ii) Associativity}
			\[ A + \l(B + C\r) = \l(A + B \r) + C\]
	\item \textit{iii) Identity}
		\[ \exists 0 \in V \]
		\[ \text{ s.t } v + 0 = v, \forall v \in V \]
	\item \textit{iv) Inverse}
		\[ \text{given } v \in V, \exists v` \in V \]
		\[ \text{s.t } v + v` = 0 \]
\end{itemize}

A set of scalars associated with a vector space is a Field \\

\subsection{Definition of Fields}

\begin{definition}[Fields]
	\[ \l(F, +, \cdot\r) \]
	\begin{itemize}
		\item $\l(F, +\r)$ Satifies i) to iv) [an abelian group]
		\item $\l(F -\{0\} , \cdot\r)$ Satifies i) to iv) [an abelian group]
		\item $\cdot$ is distributive over $+$
	\end{itemize}
\end{definition}

\begin{example}[Binary field]
	\[ \{0,1\} \]
	Addition:
	\[ 0 + 1 = 1 = 1 + 0 \]
	\[ 1 + 1 = 0 \]
	Multiplication:
	\[ 1 \cdot 0 = 0 = 0 \cdot 1\]
	\[ 1 \cdot 1 = 1 \]
	\[ 0 \cdot 0 = 0 \]
\end{example}

What a field does to a vector in a vector space:

$\g, \mu \in F$, $v, w \in V$

\[ \g \cdot \l( v + w \r) = \l( \g v + \g w \r) \]

\[ \l( \g + \mu \r) v = \g v + \mu v \]

\[ \mu \l( \g \cdot v \r) = \l( \mu \g \r) v \]

Properties of a vector space $V$ over field $F$:

$u, v, w \in V$ and $a, b \in F$

\begin{enumerate}
	\item $ u + v = w = v + u $
	\item $ u + \l(v + w\r) = \l(u + v \r) + w$
	\item $ \exists 0 \in V, \text{s.t } 0 + v = v $
	\item $ \exists v` \in V, \text{s.t } v + v` = 0 $
	\item $ \exists 1 \in F, \text{s.t } 1 \cdot v = v $
	\item $ \l(ab\r) \cdot v = a\cdot \l(bv\r) $
	\item $ a \cdot \l( v+w\r) = a \cdot v + a \cdot w $
	\item $ \l(a+b\r) \cdot v = a\cdot v + b \cdot v $
\end{enumerate}

\begin{example}
	set $V = \{0\}$ over the field $\R$
\end{example}

\begin{example}
	set $\R, \R^2, \R^3 \dots \R^n$ over the field $\R$
\end{example}

\begin{remark}
	If $F$ is a field, then $F$ is also a Vector space over itself
\end{remark}

\subsection{Definition of Vector Spaces}

\begin{definition}[Vector Space]
	\[ \l(V,F,+,\cdot\r) \]
	\begin{itemize}
		\item $\l(V,+\r)$ is an Abelian group
		\item $ \cdot : F \times V \rightarrow V $ is associative and follows distributivity
	\end{itemize}
\end{definition}

\begin{example}
	\[ \Q, \R, \C \text{ are all fields}\]
\end{example}

\begin{proposition}
	Any field $ F\subset \R$ contains $\Q$
\end{proposition}

\begin{proof}
	We assume normal addition and multiplication
	\[ \forall a \in \Q, a = \frac{p}{q} \]
	where $p,q \in \Z$\\
	$0,1 \in F$, as $\l(F, +\r)$ and $\l(F-\{0\}, \times\r)$ is an abelian groups. 
	As $+$ is closed $1+1 = 2 \in F$ and $2+1=3 \in F$ and so on. 
	By inverse property of $+$, we know $-1,-2,-3 \dots\in F$, therefore $\Z\in F$. 
	By inverse property of $\times$, we know $\{1/a: a\in\Z\}\in F$.
	Since $\times$ is closed, $S=\{a/b: a,b \in\Z\}\in F$, set $S$ is just $\Q$. Therefore $Z\in F$ if $F \subset\R$ is a field.
	
\end{proof}

Properties of fields:

\begin{enumerate}
	\item \textit{Cancellation law,}
		If $a+b = a+c$ then $b=c$
	\item Additive identity is unique
	\item Additive inverse is unique
	\item Multiplicative identiy is unique
	\item Multiplicative inverse is unique
	\item 
		if $a\cdot b = a \cdot c$ then $b=c$ if $a \neq 0$
\end{enumerate}

Properties of Vector spaces:

\begin{enumerate}
	\item \textit{Cancellation law,}
		If $a+b = a+c$ then $b=c$
	\item Additive identity is unique
	\item Additive inverse is unique
	\item $0\cdot v = 0\rightarrow$ is the null vector
\end{enumerate}

Matrices as Vector spaces:

\[ M_{n\times m}\l(\R\r) \]

\begin{itemize}
	\item Addition is entry wise
	\item $0_{n\times m}$ the additive identity
	\item Scaling is entry wise
\end{itemize}

$M_{n\times m}$ looks like $\R^{n.m}$

\subsection{Vector Subspaces}

\begin{definition}[Vector Subspace]
	$W$ is said to be a vector subspace of $V$ if 
	\begin{itemize}
		\item $ W \subseteq V$
		\item $W$ is a vector subspace
	\end{itemize}
\end{definition}

\begin{example}
	$P\l(\R\r)$ is the set of all polynomials.\\
	$P_n\l(\R\r)$ is the set of all polynominals of degree at most n.
	\begin{itemize}
		\item $ P_0\l(\R\r) \subset P_1\l(\R\r) \subset P_1\l(\R\r) \subset P_3\l(\R\r) \subset \dots P_n\l(\R\r) $
		\item $\{a_0 + a_2 x^2 + a_4 x^4 \dots : a_i \in \R\} $
	\end{itemize}
\end{example}

\begin{example}
	$M_n\l(\R\r)$ is the set of all $n\times n$ matrices \\
	\begin{itemize}
		\item Set of all symmetric matrices, $ S = \{ A : A \in M_n\l(\R\r), A = A^t \} $
		\item $ T = \{ A : A \in M_n\l(\R\r), Tr\l(A\r)=0\} $
	\end{itemize}
\end{example}

\section{Span and Linear Independence}

\subsection{Definiton of Span}

\begin{definition}[Span]
	\[ \text{span}\l(S\r) = \{ \g_1 v_1 + \g_2 v_2 \dots \g_k v_k : v_i\in S, \g_i \in F \} \]
\end{definition}

\begin{example}
	\[ S = \{1,x,x^2\} \]
	\[ \text{span}\l(S\r) = P_2\l(\R\r) \]
\end{example}

\begin{example}
	\[ S = \{\l(1,0,0\r),\l(0,1,0\r),\l(0,0,1\r)\} \]
	\[ \text{span}\l(S\r) = \R^2 \]
\end{example}

\begin{example}
	\[ S = \{ \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix},
		  \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix},
		  \begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix}	\} \]
	\[ \text{span}\l(S\r) = M_2\l(\R\r)_{sym} \]
\end{example}

Properties of Span:

\begin{enumerate}
	\item span$\l(S\r)$ is always a subspace containing $S$ as,\\
		$+$ is closed and\\
		Scaling operation is also closed.
	\item For a subspace $S\subseteq W$, $S\subseteq \text{span}\l(S\r) \subseteq W$
	\item $\text{span}\l(S\r)$ is the smallest subspace containg S.\\
	      That is, $\text{span}\l(S\r)$ is the intersection of all subspaces containing $S$.
        \item $\text{span}\l(\es\r) = \{ 0\}$\\
	      This is more of a convention, or you can arrive at this by following the above definition of Span.
      \item $S_1 \subseteq S_2$ implies $\text{span}\l(S_1\r) \subseteq \text{span}\l(S_2\r)$
      \item $\text{span}\l(\text{span}\l(S\r)\r) = \text{span}\l(S\r) $ 
      \item $\text{span}\l(W\r) = W$ if and only if $W$ is a subspace.
      \item $\text{span}\l(S_1\cup S_2\r) = \text{span}\l(S_1\r) +\text{span}\l(S_2\r)$\\
	      where $A+B=\{a+b:a\in A,b\in B\}$
\end{enumerate}

\begin{remark}
	if $V_1$ and $V_2$ are subspaces then, $V_1+V_2$ is also a subspace.
\end{remark}

\subsection{Linear (In)dependence}

Given a non empty set $S$, consider $\text{span}\l(S\r)$. Is there an element $v\in S$ such that, $\text{span}\l(S\r) = \text{span}\l(S-\{v\}\r)$?\\

Then $S$ is linearly dependent.

\begin{example}
	\[ S = \{\l(0,0,0\r),\l(0,1,0\r),\l(0,0,e\r)\} \]
	\[ \text{span}\l(S\r) = \{\l(0,a,b\r): a,b\in\R\} = \text{span}\l(S-\{\l(0,0,0\r)\}\r)\]
\end{example}

For $\text{span}\l(S\r) = \text{span}\l(S-\{v\}\r)$, there exists $v_1,v_2, \dots,v_k \in S -\{v\}$ and scalars $\g_1,\g_2,\dots,\g_k \in F$ such that,

\[ V = \sum_{i=1}^k \g_i v_i \]

as $0\notin S$, not all $\g_i$ can be $0$.

\begin{definition}[Linear Dependence]
	A subset $S\subseteq V$ is said to be linearly dependent if there exists scalars, $\g_1,\g_2,\dots\g_k\in F$ such that for $v_1,v_2,\dots,v_k\in S$
	\[\g_1 v_1 + \g_2 v_2 + \dots + \g_k v_k = 0 \]

	Where not all $\g_i = 0$ and $v_i$ are all distinct.
\end{definition}

\begin{definition}[Linear Independence]
	A subset $S\subseteq V$ is said to be linearly independent if there exists no scalars, $\g_1,\g_2,\dots\g_k\in F$ such that for $v_1,v_2,\dots,v_k\in S$
	\[\g_1 v_1 + \g_2 v_2 + \dots + \g_k v_k = 0 \]

	Where not all $\g_i = 0$ and $v_i$ are all distinct.
\end{definition}

\begin{example}
	$V = P\l(\R\r) $, $S=\{1+x,1-x,x^2,2x^2-1\}$
	\[ \frac{1}{2} \l(1+x\r) + \frac{1}{2}\l(1-x\r) + \l(2x^2-1\r) + \l(-2\r)x^2 = 0 \]
	$S` =\{1+x,1-x,x^2\}$
\end{example}

\begin{remark}
	if $S$ is linearly dependent, then $\exists v\in S$ such that $\text{span}\l(S\r) = \text{span}\l(S-\{v\}\r)$ 
\end{remark}

\begin{proposition}
	if $S$ is linearly independent and $\exists v \notin \text{span}\l(S\r)$ then $S\cup\{v\}$ is linearly independent.
\end{proposition}

\begin{proof}
	let $\g,\g_1,\dots,\g_k \in F$ and $v_1,\dots,v_k \in S$
	\[ \g_1 v1 + \dots + \g_k v_k + \g v = 0\]
	case 1: $\g = 0$,
	\[ \g_1 v1 + \dots + \g_k v_k  = 0\]
	as $S$ is linearly independent $\g_i = 0$ therefore,
	\[ \g_1,\g_2,\dots,\g_k,\g = 0 \]
	therefore $S\cup\{v\}$ is linearly independent.\\
	case 2: $\g \neq 0$,
	\[ v = \frac{-\g_1}{\g} v_1 + \dots + \frac{-\g_k}{\g}v_k \]
	but $v\notin \text{span}\l(S\r)$, therefore this case is not possible.
\end{proof}

\section{Basis and Dimension of a vector space}

\subsection{Basis and Dimension}

\begin{definition}[Basis]
	set $\b\subseteq V$ is called the basis of $V$ if
	\begin{enumerate}
		\item $\b$ is linearly independent.
		\item $\b$ spans $V$
	\end{enumerate}
\end{definition}

\begin{definition}[Dimension]
	For $\b$ being a basis of $V$, $\text{dim}_FV = |\b|$
\end{definition}

\subsection{Replacement Theorem}

\begin{theorem}[Replacement Theorem]
	    Let $S\subseteq V$ be a finite set which spans $V$ and let $L\subseteq V$  be a finite linearly independent subset, then,
	    \begin{enumerate}
		    \item $|L| \leq |S|$
		    \item $\exists T \subseteq S$ (T is of size $|S|-|L|$) such that $T\cup L$ spans V
	    \end{enumerate}
\end{theorem}

\begin{proof}
	\[ L = \{ u_1,\dots,u_m\} \]
	\[ S = \{ v_1,\dots,v_n\} \]
	\[ u_m \in V = \text{span}\l(S\r) \]
	therefore,
	\[ u_m = a_1 v_1 + \dots + a_n v_n \]
	as $u_m$ is in $L$ a linearly independent set, $u_m$ cannot be $0$, therefore, all of $a_i$ cannot be zero as well. Let $a_n \neq 0$.
	\[ v_n = \frac{1}{a_n}u_m +\frac{-a_1}{a_n}v_1 +\dots + \frac{-a_{n-1}}{a_n}v_{n-1} \]
	\[ v_n \in \text{span}\{v_1,\dots,v_{n-1},u_m \} = \text{span}\l(S\r) = V \]
	similarly,
	\[ u_{m-1} \in V = \text{span}\{v_1,\dots,v_{n-1},u_m \}  \]
	therefore,
	\[ u_{m-1} = a_1 v_1 + \dots + a_{n-1} v_{n-1} + b_m u_m  \]
	as $u_{m-1}$ and $u_m$ are linearly independent, therefore all of $a_i$ cannot be $0$
	\[ v_{n-1} = \frac{1}{a_{n-1}}u_{m-1} +\frac{-b_m}{a_{n-1}}u_m + \frac{-a_1}{a_{n-1}} v_1 \dots + \frac{-a_{n-2}}{a_{n-1}}v_{n-2} \]
	\[ v_{n-1} \in \text{span}\{v_1,\dots,v_{n-1},u_{m-1},u_m \} = \text{span}\l(S\r) = V \]
	Now iterate this process to get,
	\[ S` = \{ v_1,\dots,v_{n-m},u_1,\dots,u_m \} \]
	Where $S`$ spans $V$,
	\[ \text{span}\l(S`\r) = V \]
	If $n<m$ then, there will be a point when iterating the steps where a proper subset of L will span V, which would not be possible since this breaks the linear independence of L.\\
	The set $T$ from the theorem is,
	\[ T = S` - L \]
\end{proof}

\begin{remark}
	If $S\subseteq V$ spans $V$ and $L\subseteq V$ is an linearly independent subset, then $|L|\leq |S|$.
\end{remark}

\begin{corollary}
	Given 2 basis of the same vector space $V$, $\b$ and $\b`$.
	\[ |\b|=|\b`|\]
\end{corollary}

\begin{proof}
	Set $S = \b$ and $L = \b`$. $\b$ spans $V$ by definition of basis and $\b`$ is linearly independent by definition of basis.\\
	Now according to 1) of replacement theorem, we get.
	\[ |\b| \leq |\b`| \]
	Similarly, we can swap $S$ and $L$ to get,
	\[ |\b| \geq |\b`| \]
	\[ \therefore |\b| = |\b`| \]
\end{proof}

\begin{remark}
	Going back to the definition of dimension, we can see that basis need not be unique but the dimension of a vector space is unique regardless of which basis we use to find the dimension
\end{remark}

\begin{corollary}
	Let $V$ be a vector space of finite dimension $n$, then
	\begin{enumerate}
		\item Any finite spanning set $S$, $|S|\geq n$, if $|S|=n$ then $S$ is a basis.
		\item Any linearly independent set $L$, $|L|\leq n$, if $|L|=n$, then $L$ spans $V$.
		\item Any linearly independent set $L$ can be extended to form a basis of $V$.
	\end{enumerate}
\end{corollary}

\begin{corollary}
	Let $V$ be a finite dimensional vector space over the field $F$ andlet $W\subseteq V$ be a subspace, then,
	\[ \text{dim}_F\l(W\r) \leq \text{dim}_F \l(V\r) \]
	If equality holds, then, $V=W$.
\end{corollary}

\begin{proof}
	Let $\b$ be a basis of $V$ of size $n$. Now look at span$\{\es\} = \{0\}$, if it is $W$ then we are done, if not, we choose $u_1\in W$ which is non zero, $L_1 = \{u_1\}\subseteq W$ is linearly independent. We check again if $\text{span}\l(L_1\r)=W $, if it is then we are donem, if it isn't then we choose $u_2\in W \setminus \text{span}\l(L_1\r)$. We iterate this process until we get $\b_\circ = \{u_1,\dots,u_k\}$ which spans $W$. This set is linearly independent by construction (refer to proposition 2.11). Using 1) from replacement theorem, using $\b$ as S and $\b_circ$ as L, we get,
	\[ |\b_\circ| \leq |\b| \]
	which is the same as,
	\[ \text{dim}_F\l(W\r) \leq \text{dim}_F \l(V\r) \]
	And using corollary 3.8 we can say that the equality implies $V=W$.	
\end{proof}

\begin{remark}
	Given subspace $W$ of vector space $V$, with $\b_\circ$ as the basis of $W$, we can extend the basis $\b_\circ$ to $\b$ as a basis for $V$
\end{remark}

\subsection{Direct Sum}

Let $W_1$ and $W_2$ be subspaces of a finite dimensional vector space $V$. Then $W_1+W_2$ is also a subspace.

Case A: $W_1 \cap W_2 = \{0\}$\\

\begin{definition}[Direct Sum]
	Given subspaces $W_1$ and $W_2$ of a finite dimensional vector space $V$, if $W_1 \cap W_2 = \{0\}$ then the vector subspace $W_1 + W_2$ is known as the direct sum.
\end{definition}

Case B: $W_1 \cap W_2 \neq \{0\}$

\[ \text{dim}_F\l(W_1 +W_2 \r) = \text{dim}_F\l( W_1 \r) + \text{dim}_F\l(W_2 \r) - \text{dim}_F\l( W_1\cap W_2\r) \]

\begin{proof}
	Start with basis of $W_1\cap W_2$, $\b_\circ = \{u_1,\dots,u_k\}$. Now extend this basis to a basis of $W_1$, $\b_1 = \{u_1,\dots,u_k,w_1,\dots,w_l\}$ and similarly for $W_2$, $\b_2 = \{u_1,\dots,u_k,w_1`,\dots,w_m\}$
	\[\b = \b_1 \cup \b_2 \]
	\[\b =  \{u_1,\dots,u_k,w_1,\dots,w_l,w_1`,\dots,w_m`\}\]
	As any element in $W_1 + W_2$ can be represented by a linear combination of elements of $\b$, $\b$ spans $W_1+W_2$. To show that $\b$ is linearly independent,
	\[ c_1 u_1 + \dots + c_k u_k + d_1 w_1 + \dots + d_l w_l + \g_1 w_1` + \dots + \g_m w_m` = 0 \]
	\[ => c_1 u_1 + \dots + c_k u_k + d_1 w_1 + \dots + d_l w_l = - \g_1 w_1` - \dots - \g_m w_m` = v \] 
	Clearly $v\in W_1$ and $v\in W_2$, therefore $v \in W_1 \cap W_2$, therefore $v$ can be written as,
	\[ v = c_1` u_1 + \dots + c_k` u_k \]
	\[ => c_1 u_1 + \dots + c_k u_k + d_1 w_1 + \dots + d_l w_l = - \g_1 w_1` - \dots - \g_m w_m` = v = c_1` u_1 + \dots + c_k` u_k \]
	From these 2 equations, we are able to determine that $\b$ is linearly independent,\\
	We have,
	\[ - \g_1 w_1` - \dots - \g_m w_m`  = c_1` u_1 + \dots + c_k` u_k \]
	$\{u_1,\dots,u_k,w_1`,\dots,w_m`\}$ is a linear independent set, therefore,
	\[  \g_1 = \dots = w_m`  = c_1` = \dots = c_k` = 0 \]
	Similarly, we have,
	\[ c_1 u_1 + \dots + c_k u_k + d_1 w_1 + \dots + d_l w_l = c_1` u_1 + \dots + c_k` u_k \]
	We know $c_1` = \dots = c_k` = 0$, therefore,
	\[ c_1 u_1 + \dots + c_k u_k + d_1 w_1 + \dots + d_l w_l = 0 \]
	$\{u_1,\dots,u_k,w_1,\dots,w_l\}$ is a linear independent set, therefore,
	\[  \g_1 = \dots = w_m`  = c_1 = \dots = c_l = 0 \]
	Therefore the only solution for
	\[ c_1 u_1 + \dots + c_k u_k + d_1 w_1 + \dots + d_l w_l + \g_1 w_1` + \dots + \g_m w_m` = 0 \]
	is all the scalars being equal to zero, which implies linear independence of $\b$. Therefore $\b$ is both linearly independent and spans $W_1+W_2$, which makes it a basis for $W_1+W_2$.
	\[ \text{dim}_F\l( W_1 \r) = |\b_1| = k + l \]
	\[ \text{dim}_F\l( W_2 \r) = |\b_1| = k + m \]
	\[ \text{dim}_F\l( W_1 + W_2 \r) = |\b| = k + l + m \]
	\[ \text{dim}_F\l( W_1 \cap W_2 \r) = |\b_\circ| = k  \]
	\[ \therefore\text{dim}_F\l(W_1 +W_2 \r) = \text{dim}_F\l( W_1 \r) + \text{dim}_F\l(W_2 \r) - \text{dim}_F\l( W_1\cap W_2\r) \]
\end{proof}

\begin{proposition}
	If $V$ is the direct sum, $W_1+W_2$ then $v\in V$ can be uniquely expressed as $w_1+w_2$ where $w_1\in W_1$ and $w_2\in W_2$.
\end{proposition}

\begin{proof}
	Let, $v = w_1 + w_2 = w_1` + w_2`$ then, $w_1 - w_1` = w_2 - w_2` = x$. therefore $x\in W_1 \cap W_2$ and by definition of direct sum, $x \in \{0\}$, therefore $x=0$. Which gives us, $w_1 = w_1`$ and $w_2=w_2`$.
\end{proof}

\section{Quotient Spaces}

\subsection{Quotient Spaces}

Goal is to create a new vector space out of $W\subseteq V$, define an equvalence relation $\sim _W$ on $V$. $v_1 \sim_W v_2$ if and only if $v_1-v_2\in W$

\begin{itemize}
	\item \textit{Reflexive}
		Yes, if $0\in W$.
	\item \textit{Symmetric}
		Yes, if whenever, $v_1-v_2\in W$ implies $v_2-v_1\in W$, ie, $-\l(v_1-v_2\r)\in W$.
	\item \textit{transitivity}
		Yes, if whenever $v_1-v_2\in W$ and $v_2-v_3\in W$ implies $v_1-v_3 = \l(v_1-v_2\r) + \l(v_2-v_3\r) \in W$.
\end{itemize}

Starting with any arbitary subset of $V$ we end with a subspace of $V$ due to the conditions that needs to be satisfied for $\sim_W$ to be an equivalence relation.\\

For any eqivalence relation $\sim$ on an arbitary set $S$, an equivalence class is given by.

\[ \l[s\r] = \{s`:s`\in S, s` \sim s \} \]

Equivalence relations are disjoined and partition the set. that is either $\l[s\r]\cap\l[s`\r]=\es$ or $\l[s\r]=\l[s`\r]$.\\

For $\sim_W$,

\[ \l[v\r] = \{ v_1:v_1-v\in W\} \]
\[ \l[v\r] = \{ v_1=v+w : w\in W\} \]
\[ \l[v\r] = v+W\]

$\l[v_1\r] = \l[v_2\r]$ we translate via $W$,

\[ v_1+W = v_2+W \] 
\[ v_1 = v_2 + W\]
\[ v_1 -v_2 = w\in W\]

Therefore $\l[v_1\r] =\l[v_2\r]$ if and only if $ v_1 -v_2 \in W$.\\

We can create a new vector space, where each element is an equivalence class of the equivalence relation $\sim_W$. Consider the set of all equivalence class of the relation $\sim_W$, $V/W$.\\

Addition under the vector space $V/W$,

\[ +: V/W \times V/W \rightarrow V/W \]

\[ \l[v_1\r] + \l[v_2\r] = \l[v_1+v_2\r]  \]

Scaling,

\[ \times : F \times V/W \rightarrow V/W \]

\[ \l(\g,\l[v\r]\r) \rightarrow \l[\g v\r] \]

\section{Linear Maps}

\subsection{Linear Maps}

Given vector spaces $V$, $W$ (over the same field $F$) and linear map $T$.

\[ T: V \rightarrow W \]

$T$ should be compatible with the structures of the vector spaces $\forall v_1,v_2,v\in V$ and $\forall \g \in F$.

\begin{itemize}
	\item $T\l(v_1+v_2\r)=Tv_1+Tv_2$
	\item $T\l(\g v\r)=\g Tv$
\end{itemize}

\begin{example}[Scaling Linear Map]
	\[ T:\R\rightarrow\R,x\rightarrow\g_\circ x\]
	\[ T\l(x+y\r) = \g_\circ\l(x+y\r) = \g_\circ x + \g_\circ y = Tx + Ty \]
	\[ T\l(\g y\r)= \g_\circ\l(\g y\r)= \g\l(\g_\circ y\r)=\g\l(Tx\r) \]
\end{example}

\begin{remark}
	$T\l(0_v\r) = 0_w$
\end{remark}

\begin{example}[Dilation]
	\[ T:V \rightarrow V,v\rightarrow 2v \l(=v+v\r)\]
	\[ \g v \rightarrow 2 \l(\g v\r) = \g\l(2v\r) = \g\l(Tv\r) \]
	If in the field, $2=0$ then the map is the zero map, if not then it's a bijective map.
\end{example}

\begin{example}[Identity map]
	\[ I:V \rightarrow V, v\rightarrow v\]
\end{example}

\begin{example}[Trivial map]
	\[ T_0:V\rightarrow W, v \rightarrow 0_w \]
\end{example}

\begin{example}[Matrices]
	\[ A \in M_{n\times n}\l(\R\r) \]
	\[ L_A: \R^n \rightarrow \R^n \]
	\[ \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} 
	   \rightarrow A \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}\]
	\[ L_A\l(\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} + \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} \r) = L_A \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} + L_A \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} \]
	\[ L_A \l(\g\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \r)\rightarrow \g L_A \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}\]
\end{example}

\begin{example}[Reflection]
	\[ T:\R^2 \rightarrow \R^2,\l(x,y\r)\rightarrow\l(x,-y\r) \]
	This is for reflection across x-axis, for any other reflection the line as to pass through the origin because $0$ has to be mapped to 0 and in general $T.T$ gives us the identity map.
\end{example}

\begin{example}[Rotation]
	\[ T:\R^2\rightarrow\R^2,\l(x,y\r)\rightarrow \l(-y,x\r) \]
	This is rotaion of $\pi /2$ clockwise, for a more general rotaion,
	\[ T_\theta : \R^2 \rightarrow \R^2 \]
	\[ \begin{pmatrix} x\\y\end{pmatrix} \rightarrow
	   \begin{pmatrix} \text{cos }\theta &-\text{sin }\theta\\
	   \text{sin }\theta & \text{cos }\theta \end{pmatrix} 
	   \begin{pmatrix} x\\y\end{pmatrix}\]
	\[ ||T\l(x,y\r)|| =||\l(x,y\r)|| \]
	This is a property of every rotation linear map.
\end{example}

\begin{example}[Projection]
	\[ P:\R^2\rightarrow\R^2,\l(x,y\r)\rightarrow\l(x,0\r)\]
	\[ Q:\R^2\rightarrow\R^2,\l(x,y\r)\rightarrow\l(x-y,0\r)\]
	\[ P^2 = P.P = P \]
	\[ Q^2 = Q.Q = Q \]
	This is the defining property of a projection map.
\end{example}

\begin{example}[Inclusion]
	\[ T:\R^2\rightarrow\R^3,\l(x,y\r)\rightarrow\l(x,y,0\r)\]
	\[ T`:\R^2\rightarrow\R^3,\l(x,y\r)\rightarrow\l(x,y,ax+by\r)\]
\end{example}

\subsection{Null Space}

\begin{definition}[Null Space]
	Given $T:V\rightarrow W $, null space $N\l(T\r)$ is given by,
	\[ N\l(T\r) = \{ v \in V: T\l(v\r) = 0_w \} \]
\end{definition}

\begin{example}[Scaling Linear Map]
	\[ T:\R\rightarrow\R,x\rightarrow\g_\circ x\]
	\[ N\l(T\r) = \{ 0\} \]
\end{example}

\begin{example}[Dilation]
	\[ T:V \rightarrow V,v\rightarrow 2v \l(=v+v\r)\]
	\[ N\l(T\r) = \begin{matrix} V & 2 = 0 \text{ in } F \\ \{ 0 \} & 2 \neq 0 \text{ in } F \end{matrix} \]
\end{example}

\begin{example}[Identity map]
	\[ I:V \rightarrow V, v\rightarrow v\]
	\[ N\l(T\r) = \{ 0 \} \]
\end{example}

\begin{example}[Trivial map]
	\[ T_0:V\rightarrow W, v \rightarrow 0_w \]
	\[ N\l(T\r) = V \]
\end{example}

\begin{example}[Reflection]
	\[ T:\R^2 \rightarrow \R^2,\l(x,y\r)\rightarrow\l(x,-y\r) \]
	\[ N\l(T\r) = \{0\} \]
\end{example}

\begin{example}[Rotation]
	\[ T_\theta : \R^2 \rightarrow \R^2 \]
	\[ \begin{pmatrix} x\\y\end{pmatrix} \rightarrow
	   \begin{pmatrix} \text{cos }\theta &-\text{sin }\theta\\
	   \text{sin }\theta & \text{cos }\theta \end{pmatrix} 
	   \begin{pmatrix} x\\y\end{pmatrix}\]
	\[ N\l(T\r) = \{0\} \]
\end{example}

\begin{example}[Projection]
	\[ P:\R^2\rightarrow\R^2,\l(x,y\r)\rightarrow\l(x,0\r)\]
	\[ N\l(P\r) = \{\l(0,y\r)\} \]
	\[ Q:\R^2\rightarrow\R^2,\l(x,y\r)\rightarrow\l(x-y,0\r)\]
	\[ N\l(Q\r) = \{\l(x,y\r):x=y\} \]
\end{example}

\begin{example}[Inclusion]
	\[ T:\R^2\rightarrow\R^3,\l(x,y\r)\rightarrow\l(x,y,0\r)\]
	\[ N\l(T\r) = \{0\} \]
	\[ T`:\R^2\rightarrow\R^3,\l(x,y\r)\rightarrow\l(x,y,ax+by\r)\]
	\[ N\l(T\r) = \{0\} \]
\end{example}

\subsection{Range}

\begin{definition}[Range]
	Given $T:V\rightarrow W $, null space $R\l(T\r)$ is given by,
	\[ R\l(T\r) = \text{Image of $V$ under $T$} \] 
\end{definition}

\begin{example}[Scaling Linear Map]
	\[ T:\R\rightarrow\R,x\rightarrow\g_\circ x\]
	\[ R\l(T\r) = \R \]
\end{example}

\begin{example}[Dilation]
	\[ T:V \rightarrow V,v\rightarrow 2v \l(=v+v\r)\]
	\[ R\l(T\r) = \begin{matrix} \{0\} & 2 = 0 \text{ in } F \\ V & 2 \neq 0 \text{ in } F \end{matrix} \]
\end{example}

\begin{example}[Identity map]
	\[ I:V \rightarrow V, v\rightarrow v\]
	\[ R\l(T\r) = V \]
\end{example}

\begin{example}[Trivial map]
	\[ T_0:V\rightarrow W, v \rightarrow 0_w \]
	\[ R\l(T\r) = \{0\} \]
\end{example}

\begin{example}[Reflection]
	\[ T:\R^2 \rightarrow \R^2,\l(x,y\r)\rightarrow\l(x,-y\r) \]
	\[ R\l(T\r) = \R^2\]
\end{example}

\begin{example}[Rotation]
	\[ T_\theta : \R^2 \rightarrow \R^2 \]
	\[ \begin{pmatrix} x\\y\end{pmatrix} \rightarrow
	   \begin{pmatrix} \text{cos }\theta &-\text{sin }\theta\\
	   \text{sin }\theta & \text{cos }\theta \end{pmatrix} 
	   \begin{pmatrix} x\\y\end{pmatrix}\]
	\[ R\l(T\r) = \R^2 \]
\end{example}

\begin{example}[Projection]
	\[ P:\R^2\rightarrow\R^2,\l(x,y\r)\rightarrow\l(x,0\r)\]
	\[ R\l(P\r) = \{\l(x,0\r)\} \]
	\[ Q:\R^2\rightarrow\R^2,\l(x,y\r)\rightarrow\l(x-y,0\r)\]
	\[ R\l(Q\r) = \{\l(x,0\r)\} \]
\end{example}

\begin{example}[Inclusion]
	\[ T:\R^2\rightarrow\R^3,\l(x,y\r)\rightarrow\l(x,y,0\r)\]
	\[ R\l(T\r) = \{\l(x,y,0\r)\} \]
	\[ T`:\R^2\rightarrow\R^3,\l(x,y\r)\rightarrow\l(x,y,ax+by\r)\]
	\[ R\l(T\r) = \{\l(x,y,ax+by\r)\} \]
\end{example}

\subsection{Null space and Range as vector subspaces}

\[ v_1,v_2\in N\l(T\r),\g\in F \]

\[ T\l(v_1+v_2\r) = Tv_1 + Tv_2 \]

\[ T\l(v_1+v_2\r) = 0 + 0 = 0 \]

\[ \therefore v_1+v_2 \in N\l(T\r)  \]

\[ T\l(\g v_1\r) = \g Tv_1 = \g 0 = 0 \]

\[ \therefore \g v_1 \in N\l(T\r) \]

Therefore $ N\l(T\r)$ is a subspace.

\[ w_1,w_2\in R\l(T\r),\g\in F \]

\[ w_1 = Tv_1 \]

\[ w_2 = Tv_2 \]

\[ w_1+w_2 = Tv_1 + Tv_2 = T\l(v_1+v_2\r) \]

\[ \therefore w_1+w_2 \in R\l(T\r)  \]

\[ w_1 = Tv_1 \]

\[ \g Tv_1 = \g w_1 = T\l(\g v_1\r) \]

\[ \therefore \g w_1 \in R\l(T\r) \]

Therefore $ R\l(T\r)$ is a subspace.

\begin{example}[Differentiation]
	\[ D:P\l(\R\r) \rightarrow P\l(\R\r),P\rightarrow P` \]
	\[ a_0+a_1x+\dots+a_kx^k \rightarrow a_1+2a_2x+\dots+ka_kx^{k-1} \]
	\[ D.\l(P_1+P_2\r) = D.P_1 + D.P_2   \]
	\[ D.\l(\g P_1\r) = \g D.P_1    \]
	\[ D:P_n\l(\R\r) \rightarrow P_n\l(\R\r) \]
	\[ N\l(D\r) = \{a_0 \in\R\} \]
	\[ R\l(D\r) = P_{n-1}\l(\R\r) \]
\end{example}

\begin{example}[Integration]
	\[ I:P\l(\R\r) \rightarrow P\l(\R\r),P\rightarrow \int P dx \]
	\[ a_0+\dots+a_kx^k \rightarrow a_0x+\dots+\frac{a_kx^{k+1}}{k+1} \]
	\[ N\l(I\r) = \{0 \} \]
	\[ R\l(I\r) = \{J\l(x\r)\in P_{n+1}\l(\R\r):J\l(0\r)=0\} \]
\end{example}

\begin{definition}[Nullity]
	Given a linear map, nullity of the linear map is the dimension of the null space.
\end{definition}

\begin{definition}[Rank]
	Given a linear map, rank of the linear map is the dimension of the range.
\end{definition}

\subsection{Rank Nullity Theorem}

\begin{theorem}[Rank Nullity]
	Let $V$ be a finite dimensional vector space and $W$ be any vector space, if $T:V\rightarrow W$ is a linear map then,
	\[ \text{dim}_F\l(R\l(T\r)\r) + \text{dim}_F\l(N\l(T\r)\r) = \text{dim}_FV \]
\end{theorem}

\begin{proof}
	Let $\b_\circ = \{v_1,\dots,v_k\}$ be a basis of $N\l(T\r)$, now extend this to a basis of $V$, $\b$  
	\[\b=\{v_1,\dots,v_k,v_1`,\dots,v_l`\} \]
	We know,
	\[ T\l(\b\r) = \{0_w,T\l(v_1`\r),\dots,T\l(v_l`\r)\} \]
	\[ L = \{T\l(v_1`\r),\dots,T\l(v_l`\r)\} \]
	Let's take,
	\[ \g_1 T\l(v_1`\r)+\dots+\g_l T\l(v_l`\r) = 0 \]
	\[ T\l(\g_1v_1`+\dots+\g_lv_l`\r)=0\]
	\[ \g_1v_1`+\dots+\g_lv_l`\in N\l(T\r) \]
	\[ \g_1v_1`+\dots+\g_lv_l` = \g_1`v_1+\dots+\g_l`v_k \]
	\[ \g_1v_1`+\dots+\g_lv_l` - \g_1`v_1-\dots-\g_l`v_k = 0 \]
	By the linear independence of $\b$ we can conclude that,
	\[ \g_1=\dots=v_l` =\g_1`=\g_l` = 0 \]
	Which gives us that $L$ is linearly independent and this is a set which spans $R\l(T\r)$ which makes it a basis of $R\l(T\r)$
	\[ \text{dim}_F\l(R\l(T\r)\r) + \text{dim}_F\l(N\l(T\r)\r) = |L| + |\b_\circ| = l + k \]
	\[ \text{dim}_FV = |\b|=l + k  \]
	\[\therefore \text{dim}_F\l(R\l(T\r)\r) + \text{dim}_F\l(N\l(T\r)\r) = \text{dim}_FV \]
\end{proof}

\begin{remark}
	Given a linear map $T:V\rightarrow W$, if $S\subseteq V$ spans $V$, then $T\l(S\r)$ spans $R\l(T\r)$.
\end{remark}

\begin{proof}
	Given $w=T\l(v\r)\in R\l(T\r)$, express $v=\sum \g_iv_i$, where $v_i\in S$ and $\g_i\in F$. $T\l(v\r)=\sum \g_iT\l(v_i\r) \implies T\l(v\r)\in\text{span}\l(T\l(S\r)\r)$
\end{proof}

\begin{corollary}
	Given a linear map, $T:V\rightarrow V$, where $V$ is a finite dimensional vector space, then the following are equivalent.
	\begin{enumerate}
		\item $T$ is injective
		\item $T$ is surjective
	\end{enumerate}
\end{corollary}

\begin{proof}
	$T$ is injective implies, $N\l(T\r)=\{0\}\implies \text{Nullity}=0\implies \text{Rank} = \text{dim}_FV-0=\text{dim}_FV\implies R\l(T\r)=V\implies T$ is surjective.\\
	$T$ is surjective implies, $R\l(T\r)=V\implies \text{rank}= \text{dim}_FV\implies \text{Nullity}=0\implies N\l(0\r) =\{0\}$.\\
	If $Tv_1=Tv_2$ and $N\l(0\r) =\{0\}$ , then, $T\l(v_1-v_2\r)=0\implies v_1-v_2\in N\l(T\r)\implies v_1-v_2 \in \{0\} \implies v_1=v_2$.\\
	Therefore, $T$ is surjective implies injectivity as well.
\end{proof}

\begin{remark}
	As part of the above proof, we also proved that Null space being zero implies injectivity.
\end{remark}


\section{Isomorphism, Matrices and Linear maps as vector spaces}

\subsection{Linear Isomorphism}

\begin{definition}[Linear Isomorphism]
	A linear map $T:V\rightarrow W$ between vector spaces $V$ and $W$ both over a common field $F$ is called an isomorphism if $T$ is bijective.
\end{definition}

\begin{example}
	\[T:P_2\l(\R\r)\rightarrow \R^3, a+bx+cx^2\rightarrow\l(a,b,c\r)\]
	Zero in $\R^3$ is $\l(0,0,0\r)$, clearly only polynominal which is mapped to it is 0. Therefore $N\l(T\r) = \{0\}$ and Nullity $=0$. If $Tp_1=Tp_2$, then, $T\l(p_1-p_2\r)=\l(0,0,0\r) \implies T\l(a_1-a_2+\l(b_1-b_2\r)x+\l(c_1-c_2\r)x^2\r)=\l(0,0,0\r) \implies\l(a_1-a_2,b_1-b_2,c_1-c_2\r)=\l(0,0,0\r)\implies a_1=a_2,b_1=b_2,c_1=c_2\implies p_1=p_2$, therefore $T$ is injective. Rank $=$ dim$_FP_2\l(\R\r) - 0 = 3 \implies R\l(T\r)=\R^3$, therefore $T$ is surjective. Therefore $T$ is bijective.
\end{example}

\begin{remark}
	When we say a vector space looks like another vector space, we mean that there exists a linear isomorphism from one vector space to another, ie $V$ is isomorphic to $W$ as a vector space.
\end{remark}

\begin{example}
	\[ T: M_2 \l( \R \r) \rightarrow \R^4 \]
	\[ \begin{pmatrix} a & b \\ c & d \end{pmatrix} \rightarrow
	   \begin{pmatrix} a \\b \\ c \\d \end{pmatrix} \]
\end{example}

Properties of isomorphic linear maps (we will take $T:V\rightarrow W$ as our isomorphic linear map to discuss the properties):

\begin{enumerate}
	\item $T^{-1}:W\rightarrow V$ is a linear map
		\begin{proof}
			For $w_1,w_2\in W$ there exists unique $v_1,v_2\in V$ such that, $Tv_1=w_1$ and $Tv_2=w_2$ because of bijectivity of $T$, there also exists unique $v\in V$ such that, $Tv=w_1+w_2$. 
			\[T^{-1}w_1=v1,T^{-1}w_2=v_2\] 
			\[ \implies v_1+v_2=T^{-1}w_1+T^{-1}w_2\] 
			\[ \implies T\l(v_1+v_2\r)=T\l(T^{-1}w_1+T^{-1}w_2\r)\] 
			\[\implies T\l(v_1+v_2\r)=T\l(T^{-1}w_1\r)+T\l(T^{-1}w_2\r) = w_1+w_2 \]
			\[ \implies T\l(v_1+v_2\r) = w_1+w_2 \]
			Therefore unique $v=v_1+v_2$. Therefore $T^{-1}\l(w_1+w_2\r)=T^{-1}w_1+T^{-1}w_2$.\\
			For $w\in W$ there exists unique $v\in V$ such that, $Tv=w$ because of bijectivity of $T$, there also exists unique $v`\in V$ such that, $Tv=\g w$. 
			\[T^{-1}w=v\] 
			\[\g T^{-1}w=\g v\] 
			\[ \implies T\l(\g v\r)=T\l(\g T^{-1}w\r)\] 
			\[\implies T\l(\g v\r)=\g T\l(T^{-1}w\r) = \g w \]
			\[\implies T\l(\g v\r) = \g w \]
			Therefore unique $v`=\g v$. Therefore $T^{-1}\l(\g w\r)=\g T^{-1}w$\\
			Showing that addition and scaling is closed is suffice to show that $T^{-1}$ is a linear map.
		\end{proof}
	\item $\g T:V\rightarrow W, V\rightarrow \g Tv$ is an isomorphic linear map.
		\begin{proof}
			Take $v_1,v_2\in V$, such that,
			\[ \g Tv_1=\g Tv_2 \]
			\[ \implies \g T\l(v_1-v_2\r) = 0 \]
			\[ \implies T\l(v_1-v_2\r) = 0 \]
			\[ \implies v_1 = v_2 \]
			This implies injectivity of $\g T$.\\
			Suppose $\g T$ is not surjective, then $\exists w\in W$ such that, $\forall v\in V$,
			\[ \g Tv \neq w \]
			\[ Tv \neq w/\g \]
			But this is a contradiction given we know $T$ to be a bijection, therefore $\g T$ has to be surjective, therefore is bijective and an isomorphism.
		\end{proof}
	\item 
		\begin{itemize}
			\item \textit{a)} If $T:V\rightarrow W$ is an injective linear map and $\{v_1,\dots,v_n\}$ is a linearly independent set, then $\{Tv_1,\dots,Tv_n\}$ is also a linearly independent set.
				\begin{proof}
					\[ \g_1Tv_1 + \dots + \g_nTv_2 = 0 \]
					\[ \implies T\l(\g_1v_2 + \dots + \g_nv_n\r) = 0\]
					Injectivity of the linear map implies,
					\[ \g_1v_2 + \dots + \g_nv_n = 0\]
					Linear independence of $\{v_1,\dots,v_n\}$
					\[ \g_1 = \dots = \g_n = 0 \]
				\end{proof}
			\item \textit{b)} If $T:V\rightarrow W$ is an surjective linear map and let $\{w_1,\dots,w_n\}$ be a subset of $W$ and choose $v_i\in V$ such that $Tv_i = w_i$ and if $\{v_1,\dots,v_n\}$ spans $V$, then $\{w_1,\dots,w_n\}$ spans $W$.
				\begin{proof}
					We know for a given $w\in W$ there exists at one $v\in V$ such that, $Tv=w$.
					\[ v = \g_1v_1+\dots+\g_nv_n\]
					\[ Tv = T\l(\g_1v_1+\dots+\g_nv_n\r)\]
					\[ w = T\l(\g_1v_1\r)+\dots+T\l(\g_nv_n\r)\]
					\[ w = \g_1 Tv_1 + \dots + \g_n Tv_n \]
					\[ w = \g_1 w_1 +\dots+ \g_n w_n \]
					Since we choose an arbitart $w$, $\{w_1,\dots,w_n\}$ spans the set $W$.
				\end{proof}
		\end{itemize}
\end{enumerate}

\subsection{Linear maps as vectors}

\begin{convention}
	$\L\l(V,W\r)$ represents set of all linear maps from $V\rightarrow W$ and $\L\l(V\r)$ represents set of all linear maps from $V\rightarrow V$.
\end{convention}

\[ +:\L\l(V,W\r) \times \L\l(V,W\r) \rightarrow \L\l(V,W\r) \]

\[ \l(T_1+T_2\r): V \rightarrow W, v \rightarrow T_1v+T_2v \]

\[ \l(T_1+T_2\r) \in \L\l(V,W\r) \]

\[ \times :F \times \L\l(V,W\r) \rightarrow \L\l(V,W\r) \]

\[ \l(\g T\r): V \rightarrow W, v \rightarrow \g Tv \]

\[ \l(\g T\r) \in \L\l(V,W\r) \]

\begin{example}[Dual of $V$]
	$W = \R$, where $V$ is a vector space over $R$. Represented by $V^*=\L\l(V,\R\r)$ and dim$_F\l(V^*\r)=$ dim$_F\l(V\r)$.
\end{example}

\begin{example}
	$V = \R$, where $W$ is a vector space over $R$. $\L\l(\R,W\r)$ is a vector space, any element in $\L\l(\R,W\r)$ has unique property where the map is determined by just where $1$ is mapped to. 
	\[T:\R\rightarrow W\]
	\[T\l(\g\r) = \g T\l(1\r) \]
	\[ \Phi: \L\l(\R,W\r) \rightarrow W \]
	$\Phi$ is an isomorphism.
	\begin{proof}
		For every $w\in W$ there is an unique $T_w\in\L\l(\R,W\r)$ such that, $T_w:\R\rightarrow W, \g\rightarrow\g w$ or $T_w\l(\g\r)=\g w$. set of all $T_w$ is the same as $\L\l(\R,W\r)$ since if there exists an element $T`$ which is not in the set of all $T_w$ then the property that the map is determined by where 1 is mapped to will yield a contradiction. Therefore, we have a bijective between $W$ and $\L\l(\R,W\r)$. Therefore, $\Phi$ is an isomorphism.
	\end{proof}
\end{example}

\begin{conjecture}
	dim$_F\L\l(V,W\r) =$ dim$_FV\times$ dim$_FW$
\end{conjecture}

\subsection{Ordered Basis}

\begin{definition}[Ordered Basis]
	An ordered basis of a vector space $V$ is $\b = \{v_1,\dots,v_n\}$ where $\b$ is an ordered set.
\end{definition}

\begin{example}
	$V=\R^2$, $\b =\{ \l(1,0\r),\l(0,1\r)\}$
\end{example}

\begin{example}
	$V=P_2\l(\R\r)$, $\b =\{1,x,x^2\}$
\end{example}

\subsection{Matrix Representation}

Goal is to create a matrix to represent $T:V\rightarrow W$, let $\b=\{v_1,\dots,v_m\}$ be an ordered basis of $V$, let $\a=\{w_1,\dots,w_n\}$ be an ordered basis of $W$.\\

It will be assumed you know basics of matrices, although everything will be revisted later in the notes with formal definitions.

\[ Tv_j = \sum_{i=1}^n \g_{ij}w_i \]

\[ \begin{pmatrix} Tv_1 \\ Tv_2 \\ \vdots \\ Tv_m \end{pmatrix} = 
   \begin{pmatrix} w_1  & w_2  & \cdots & w_n  \end{pmatrix} 
   \begin{pmatrix}
	  \g_{11} & \g_{12} & \cdots & \g_{1m} \\
	  \g_{21} & \g_{22} & \cdots & \g_{2m} \\
	  \vdots  & \vdots  & \ddots & \vdots \\
	  \g_{n1} & \g_{n2} & \cdots & \g_{nm} \\
   \end{pmatrix} \]

\[ \begin{pmatrix}
	  \g_{11} & \g_{12} & \cdots & \g_{1m} \\
	  \g_{21} & \g_{22} & \cdots & \g_{2m} \\
	  \vdots  & \vdots  & \ddots & \vdots \\
	  \g_{n1} & \g_{n2} & \cdots & \g_{nm} \\
\end{pmatrix} \in M_{m\times n}\l(F\r)\]

Aim is to associate a matrix $\l[T\r]^{\a}_{\b}$ for a linear map $T:V\rightarrow W$, where $\b=\{v_1,\dots,v_m\}$ is an ordered basis of $V$ and $\a=\{w_1,\dots,w_n\}$ is an ordered basis of $W$.\\

\[ S_\b:V\rightarrow F^m \]

Where $S_\b$ is given by, for any $v\in V$,

\[ v = \sum_{i=1}^m \g_iv_i \rightarrow 
   \begin{pmatrix} \g_1\\ \vdots \\ \g_m \end{pmatrix} \in F^m \]

\[ S_\a:W\rightarrow F^n \]

Where $S_\a$ is given by, for any $w\in W$,

\[ w = \sum_{i=1}^n \g_iw_i \rightarrow 
   \begin{pmatrix} \g_1\\ \vdots \\ \g_n \end{pmatrix} \in F^n \]

	   \[ T : V \rightarrow W \]	
	   \[ S_\b : V \rightarrow F^m \]	
	   \[ S_\a : W \rightarrow F^n \]	
	   \[ S_\a \cdot T \cdot S_\b^{-1}  : F^m \rightarrow F^n \]	

We now define $\l[T\r]_\b^\a$ to be the matrix associated with the map $S_\a \cdot T \cdot S_\b^{-1}  : F^m \rightarrow F^n$

\begin{conjecture}
	 $\l[Tv\r]^\a = \l[T\r]^\a_\b\l[v\r]^\b$
\end{conjecture}

\begin{example}
	\[ I:V\rightarrow V \]
	For $\b=\a$,
	\[ \l[I\r]^\a_\b = I_n \]
	For $\b = \{ e_1,e_2 \}$ and $\a = \{ e_2, e_1 \}$,
	\[ \l[I\r]^\a_\b = \begin{pmatrix} 0&1\\1&0\end{pmatrix} \]
\end{example}

\begin{example}[Differentiation]
	\[ D:P_2\l(\R\r) \rightarrow P_2\l(\R\r), P\l(x\r) \rightarrow P`\l(x\r) \]
	\[ \b = \{ 1, x, x^2 \} \]
	\[ \l[D\r]^\b_\b = \begin{pmatrix} 0&1&0\\0&0&2\\0&0&0\end{pmatrix} \]
	\[ \a = \{ 2x, 1, x^2 \} \]
	\[ \l[D\r]^\a_\b = \begin{pmatrix} 0&0&1\\0&1&0\\0&0&0\end{pmatrix} \]
\end{example}

To associate a matrix to a linear map,

\[ T: V \rightarrow W \]

\[ w \l( \in W \r) = \sum_{i=1}^n = \g_i w_i \]

$\l[ T \r]_\b^\a$ is the matrix obtaining by evaulting $T$ on $V$ with basis $\b$ with respect to $\a$.

\[ \l(\l[ T \r]_\b^\a\r)_{ij} = \g_{ij} \]

Where,

\[ Tv_J = \g_{1j} w_1 + \dots + \g_{ij} w_i +\dots + \g_{nj} w_n \]

\[ \l[Tv\r]^\a = \l[Tv\r]^\a_\b \l[v\r]^\b \]

\begin{convention}
	$\l[a\r]^b$ represents a vector $a$ represented as a linear combination of elements of the set $\b$, where $\b$ is a basis for a vector space with $a$ in it.
\end{convention}

$\b = \{ v_1,\dots,v_m\}$, represents ordered basis of $V$ and $\a = \{ w_1,\dots,w_n\}$, represents ordered basis of $W$. 

The j$^\text{th}$ column of $\l[T\r]^\a_\b$ is given by the coefficients obtained by expanding $T\l(v_j\r)$ in terms of $\a$

\begin{convention}
	Just $\l[a\r]^b$ will be used instead of $\l[a\r]^b_b$
\end{convention}

\begin{example}
	\[ I: V\rightarrow V, v \rightarrow v \] 
	Given dim$_FV=n$,
	\[ \l[I\r]_\b^\b = I_n \]
\end{example}

\begin{example}
	\[ R_\theta : \R^2 \rightarrow \R^2 \]
	Rotation anti clock-wise by $\theta$,
	\[ \begin{pmatrix}x\\y\end{pmatrix} \rightarrow 
		\begin{pmatrix} cos\theta & -sin\theta \\ sin\theta & cos\theta \end{pmatrix}
		\begin{pmatrix}x\\y\end{pmatrix}   \]

	\[ \b = \{\l(1,0\r),\l(0,1\r)\} \]
	\[ \g = \{\l(1,1\r),\l(1,-1\r)\} \]
	\[ \l[R_\theta\r]_\b = \begin{pmatrix} cos\theta & -sin\theta \\ sin\theta & cos\theta \end{pmatrix} \]
	\[ \l[R_\theta\r]_\b^\g = \l[I_2\r]^\g_\b \l[R_\theta\r]_\b^\b  \]
	\[ \l[R_\theta\r]_\b^\g = \begin{pmatrix}1&1\\1&-1\end{pmatrix}\begin{pmatrix} cos\theta & -sin\theta \\ sin\theta & cos\theta \end{pmatrix} \]
	\[ \l[R_\theta\r]_\b^\g = \begin{pmatrix} cos\theta + sin\theta & -sin\theta + cos\theta\\ cos\theta - sin\theta & -sin\theta - cos\theta \end{pmatrix} \]
	\[ \l[R_\theta\r]_\g = \begin{pmatrix} cos\theta & sin\theta \\ sin\theta & cos\theta \end{pmatrix} \]
\end{example}

\begin{remark}
	Sum of diagonal elements of $\l[R_\theta\r]_\g$ and $\l[R_\theta\r]_\b$ are the same, and so is the determinant of both matrices. 
\end{remark}

\begin{example}
	\[ D:P_n\l(\R\r) \rightarrow P_n\l(\R\r), P\l(x\r) \rightarrow P`\l(x\r) \]
	\[ \b = \{1,x,\dots,x^n\} \]
	\[ \l[D\r]_\b  = \begin{pmatrix} 
	   0 & 1 & 0 & 0 & \cdots & 0\\
	   0 & 0 & 2 & 0 & \cdots & 0\\
	   0 & 0 & 0 & 3 & \cdots & 0\\
	   0 & 0 & 0 & 0 & \ddots & \vdots\\
	   \vdots & \vdots & \vdots & \vdots &  & n\\
	   0 & 0 & 0 & 0 & \cdots & 0\\
	   \end{pmatrix}\]  
We could look at  $P_n\l(\R\r) \rightarrow P_{n-1}\l(\R\r)$ but let's stick to square matrices for now, and $P_{n-1}\l(\R\r)$ is clearly a subspace of $P_n\l(\R\r)$. \\

We can see that the sum of diagonal elements is clearly 0 and the determinant is 0 as well.

\end{example}

\begin{proposition}
	Given dim$_FV=m$ and dim$_FW=n$ and given $\b = \{v_1,\dots,v_m\}$ a basis of $V$ and $\g = \{w_1,\dots,w_m\}$ a basis of $W$.
	\[ \Phi: \L\l(V,W\r) \rightarrow M_{n\times m}\l(F\r), T\rightarrow \l[T\r]_\b^\g   \]
	$\Phi$ is an isomorphism.
\end{proposition}

\begin{remark}
	This statement says that the matrix representation of linear maps between 2 finite dimensional vector spaces is "equivalent" (i.e: isomorphic) to the linear map itself.
\end{remark}

\begin{proof}
	Is $\l[S+T\r]_\b^\g =  \l[S\r]_\b^\g + \l[T\r]_\b^\g$\\
	J$^\text{th}$column of $\l[S+T\r]_\b^\g =$ coefficient of $\l(S+T\r)v_j$\\ 
	J$^\text{th}$column of $\l[S\r]_\b^\g =$ coefficient of $Sv_j$\\ 
	J$^\text{th}$column of $\l[T\r]_\b^\g =$ coefficient of $Tv_j$\\ 
	\[ \therefore \l[S+T\r]_\b^\g = \l[S\r]_\b^\g + \l[T\r]_\b^\g \]
	
	Is $\l[\lambda T\r]_\b^\g =  \lambda \l[T\r]_\b^\g$.\\
	J$^\text{th}$column of $\l[\lambda T\r]_\b^\g =$ coefficient of $\lambda Tv_j$ = $\lambda$  coefficient of $Tv_j$ = j$^\text{th}$ coefficient of $\lambda\l[T\r]_\b^\g$.\\
	Therefore $\Phi$ is a linear map.

	Zero in $M_{n\times m}\l(F\r)$ is the zero matrix.\\
	\[ \forall v \in V, \l[v\r]^\g \times 0_{n\times m} = \l[0\r]^\b \]
	Therefore the only preimage of $0_{n\times m}$ is the zero map between $V$ and $W$, therefore the null space of $\Phi$ is $\{0\}$, now we know that $\Phi$ is injective.\\
	Given a matrix $A\in M_{n\times m}$, define $T_A:V\rightarrow W,\l[v\r]_\b \rightarrow \l[v\r]_\b A $, where$ \l[v\r]_\b A$ (a $1\times n$ matrix) represents a vector $w$ as a linear sum of the vectors in basis $\g$. $T_A$ is a well defined linear map from $V \rightarrow W$ (proof of this is trivial but skipped here), therefore $T_A\in \L\l(V,W\r)$ is a preimage for $A\in M_{n\times m}$. Therefore $\Phi$ is bijective and hence an isomorphism.
	\end{proof}

\begin{remark}
	This also proves conjecture 6.8
\end{remark}

\section{Dual spaces and Dual maps}

\subsection{Dual Spaces}

\begin{definition}[Dual Space]
	Given a finite dimensional vector space $V$ over field $F$, dual space is given by $V^* = \L\l(V,F\r)$, called as dual of V. 
\end{definition}

\[ \text{dim}_F V^* = \text{dim}_F M_{1\times n}\l(F\r) = n = \text{dim}_F V \]

There is no natural linear isomorphism between $V$ and $V^*$.

\begin{quote}
	"There cannot be a cannonical or God-given isomorphism"
\end{quote}

We can take dual of a dual, known as double dual.

\begin{quote}
	"In a sense, double duals are more well behaved"
\end{quote}

\subsection{Dual basis}

let $\b = \{v_1,\dots,v_n\}$ be a basis of $V$.

\[ v_i^*: V\rightarrow F, a_1v_1+\dots + a_nv_n\rightarrow a_i \]

\[ v_i^*v_j = \delta_{ij} \]

$\{v_1^*,\dots,v_n^*\}$ is a basis of $V^*$.

\begin{proof}
	Given $T \in V^*, T: V\rightarrow F$ and $v\in V$, where $\b = \{v_1,\dots,v_n\}$ is a basis of $V$,
	\[ Tv = T\l(a_1v_1,\dots,a_nv_n\r) \]
	\[ Tv = a_1Tv_1 + \dots + a_nTv_n \]
	\[ Tv = Tv_1a_1 + \dots + Tv_na_n \]
	\[ Tv = Tv_1v_1^*v + \dots + Tv_nv_n^*v \]
	\[ Tv = \l(Tv_1v_1^* + \dots + Tv_nv_n^*\r)v \]
	\[ T = Tv_1v_1^* + \dots + Tv_nv_n^* \]
	$Tv_i$'s are fixed scalars, therefore the set $\b^*\{v^*_1,\dots,v^*_n\}$ spans $V^*$, linear independence follows from the fact that dim$_FV^*=n$ 

\end{proof}

\begin{example}
	For fixed $v\in \R^n$,
	\[ L_v:\R^n \rightarrow \R, w \rightarrow v\cdot w = \sum_{i=1}^n v_iw_i \]
	From properties of dot product, it follows that $L_v$ is a linear map, hence $L_v \in \l(\R^n\r)^*$
\end{example}

\begin{example}
	\[ T_\text{tr}: M_n\l(\R\r), A \rightarrow \text{tr}\l(A\r) \]
	Since trace of a matrix is a linear map, $T_\text{tr}\in \l(M_n\l(\R\r)\r)^*$.
\end{example}

\subsection{Dual Maps}

\begin{definition}[Dual Map]
	Given a linear map $T:V\rightarrow W$, Dual map $T^*$ of $T$ is given by,
	\[T^*:W^*\rightarrow V^*, L \rightarrow L\cdot T \]
\end{definition}

\begin{example}
	\[ T_\g : \R \rightarrow \R, x \rightarrow \g x \]
	\[ T_\g^* : \R^* \rightarrow \R^*, \l(L:\R \rightarrow \R,x \rightarrow Lx\r) \rightarrow \l(\g L : \R \rightarrow \R, x \rightarrow \g Lx \r) \]
	We know dimension of $\R^*$ to be 1, and the identity map is not equal to the zero map, therefore it is a basis for $\R^*$
\end{example}

\begin{example}
	\[ T_\text{tr}: M_n\l(\R\r), A \rightarrow \text{tr}\l(A\r) \]
	\[ \b = \bigg\{ \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix},
			\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix},  
			\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix},
			\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} \bigg\} \]
	\[ \g = \{ 1 \} \]
	$\b$ and $\g$ are basis for $M_n\l(\R\r)$ and $\R$ respectively.
	$\g^*=\{1^*\}$ where $1^*\l(c\r) = 1^*\l(c\cdot 1\r)=c\cdot 1^*\l(c\r) = c$, therefore $\g^*$ is the dual basis of $\R$ ($1^*\l(1\r) = 1$ is by definition/construction of dual basis.)  
	
	\[ \l[T\r]^\g_\b = \begin{pmatrix} 1 & 0 & 0 & 1 \end{pmatrix} \]
	
	\[ T_\text{tr}^*: \R^* \rightarrow M_n\l(\R\r)^*, L_\lambda \rightarrow L_\lambda\cdot T_\text{tr}\]
	
	Where $L_\lambda\in \R^*$, $L_\lambda$ is given by $L_\lambda:\R\rightarrow\R,a\rightarrow\lambda a$, therefore,
	
	\[ T_\text{tr}^*: \R^* \rightarrow M_n\l(\R\r)^*, L_\lambda \rightarrow\lambda T_\text{tr}\]
	
	\[ \l[T_\text{tr}^*\r]_{\g^*}^{\b^*} = \begin{pmatrix}1\\0\\0\\1\end{pmatrix} \]

	We can see that $ \l[T_\text{tr}^*\r]_{\g^*}^{\b^*}$ is the transpose of $ \l[T_\text{tr}\r]^\g_\b$.

\end{example}
	
Now let's see proof for matrix representation of dual of a map to be the transpose of the matrix representaion of the map.

\begin{proof}

	Let $T:V\rightarrow W$ be a linear map from a finite dimensional vector space to another finite dimensional vector space. Let $\b = \{v_1,\dots,v_m\}$ be a basis of $V$ and let $\g = \{w_1,\dots,w_n\}$ be a basis of vector space $W$.

	\[ \l(\l[T\r]_\b^\g\r)_{ij} = \text{i$^\text{th}$ coefficient in the expansion of $Tv_j$ in terms of $\g$} \]
	$ \l(\l[T\r]_\b^\g\r)_{ij} = C_{ij}$ if $Tv_j = \sum_{k=i}^nC_{kj}w_k$
	\[ \l(\l[T^*\r]_{\g^*}^{\b^*}\r)_{ji} = \text{j$^\text{th}$ coefficient in the expansion of $T^*w^*_i$ in terms of $\b^*$} \]
	$ \l(\l[T^*\r]_{\g^*}^{\b^*}\r)_{ji} = \widetilde{C}_{ji}$ if $Tv_j = \sum_{k=j}^m\widetilde{C}_{ki}v^*_k$
	\[ \l(T^*w^*_i\r) v^*_j= \l(\widetilde{C}_{1i} v^*_1 + \dots + \widetilde{C}_{mi}v^*_m\r) v_j \]
	\[ \l(T^*w^*_i\r) v^*_j= \widetilde{C}_{ji} v^*_j v_j \]
	\[ \l(T^*w^*_i\r) v^*_j= \widetilde{C}_{ji} \]
	\[ \l(T^*w^*_i\r) v^*_j= w^*_i \l(Tv_j\r) = w^*_i \l( C_{1j} w_1 + \dots + C_{nj} w_n \r)  \]
	\[ \l(T^*w^*_i\r) v^*_j= C_{ij} \]
	\[ \therefore \widetilde{C}_{ji} = C_{ij} \]
	\[ \therefore \l(\l[T^*\r]_{\g^*}^{\b^*}\r)^t = \l[T\r]_\b^\g\]
\end{proof}

%\newpage

\section{Matrices}

\subsection{Expected properties of matrix multiplication}

Let $R:U\rightarrow V$, $S:V\rightarrow W$ and $T:W\rightarrow X$ be linear maps between finite dimensional vector spaces and let an ordered basis of $U$ be $\a$, of $V$ be $\b$, of $W$ be $\g$ and of $X$ be $\delta$.


\begin{itemize}

	\item \textit{i) Associativity} 
		\[ \l[T\r]_\g^\delta\l(\l[S\r]^\g_\b\l[R\r]^\b_\a \r) = \l(\l[T\r]_\g^\delta \l[S\r]^\g_\b \r) \l[R\r]^\b_\a  \]
	
	\item \textit{ii)}
		\[ \l[T\cdot S\r]^\delta_\b  = \l[T\r]_\g^\delta \l[S\r]^\g_\b  \]


\end{itemize}

ii) implies i)

\begin{proof}
	\[ \l[T\r]_\g^\delta\l(\l[S\r]^\g_\b\l[R\r]^\b_\a \r) = \l[T\r]_\g^\delta \l[S\cdot R\r]^\g_\a = \l[T\cdot\l(S\cdot R\r)\r]^\delta_\a  \]

	\[ \l(\l[T\r]_\g^\delta \l[S\r]^\g_\b \r) \l[R\r]^\b_\a  = \l[T\cdot S\r]_\b^\delta \l[R\r]^\b_\a = \l[\l(T\cdot S\r)\cdot R\r]^\delta_\a \]

\end{proof}

\begin{proof}[i)]
	
\[ S:V\rightarrow W \]
\[ T:W\rightarrow X \]

	Let $\b = \{v_1,\dots,v_l\}$ be an ordered basis of $V$, $\g = \{w_1,\dots,w_m\}$ be an ordered basis of $W$ and $\delta = \{x_1,\dots,x_n\}$ be an ordered basis of $X$.

	\[ \l[T\r]^\delta_\g \implies t_{ij} \]

	\[ \l[S\r]^\g_\b \implies s_{ij} \]
	
	\[ \l(T\cdot S\r) v_j = T\l(Sv_j\r) = T \l(\sum_{i=1}^m s_{ij} w_i \r)   \] 
	
	\[ T \l(\sum_{i=1}^m s_{ij} w_i \r)  = \sum_{i=1}^m s_{ij}Tw_i = \sum_{i=1}^m s_{ij} \l( \sum_{k=1}^nt_{ki}\r) x_k = \sum_{k=1}^n \l(\sum_{i=1}^mt_{ki}s_{ij}\r) x_k \] 

	Therefore (k,j)$^\text{th}$ entry of $\l[T\cdot S\r]^\delta_\b$ is equal to the (k,j)$^\text{th}$ entry of $\l[T\r]_\g^\delta \l[S\r]^\g_\b$

\end{proof}

\subsection{Change of coordinates}

\begin{corollary}
	Let $T:V\rightarrow V$ be a linear map from a vector space of dimension n to itself and let $\b$ and $\g$ be two ordered basis of $V$ then.

	\begin{itemize}
		\item \textit{i) } $\l[I_V\r]^\g_\b \l[I_V\r]^\b_\g = I_n$
		\item \textit{ii)} $\l[T\r]^\b_\b \l[I_V\r]^\b_\g = \l[T\r]^\b_\g$ and $\l[I_V\r]^\b_\g \l[T\r]^\b_\b  = \l[T\r]^\b_\g$
	\end{itemize}
\end{corollary}


\begin{proof}
\
\begin{itemize}
	
	\item \textit{i) } $\l[I_V\r]^\g_\b \l[I_V\r]^\b_\g = \l[I_V\cdot I_V\r]^\g_\g = I_n $	
	
	\item \textit{ii)} $\l[T\r]^\b_\b \l[I_V\r]^\b_\g = \l[T\cdot I_V\r]^\b_\g = \l[T\r]^\b_\g $ and $\l[I_V\r]^\b_\g \l[T\r]^\b_\b = \l[I_V\cdot T\r]^\b_\g = \l[T\r]^\b_\g $	

\end{itemize}

\end{proof}

\begin{definition}[Change of basis]
	The matrix $\l[I_V\r]^\g_\b$ associated to the indentity map $I_V:V\rightarrow V,v\rightarrow v$ and the two ordered basis $\b$ and $\g$ of $V$ is called the change of coordinate (or change of basis) matrix.
\end{definition}



Let $\b=\{v_1,\dots,v_n\}$ and $\g = \{v_1^`,\dots,v_n^`\}$ be an ordered basis of $V$ (a vector space over the field $F$) and define linear maps from $F^n$ to $V$,

\[ \phi_\b:F^n\rightarrow V, \l(c_1,\dots,c_n\r)\rightarrow c_1v_1+\dots +c_nv_n \]

\[ \phi_\g:F^n\rightarrow V, \l(c_1,\dots,c_n\r)\rightarrow c_1v_1^`+\dots +c_nv_n^` \]

The above 2 map are linear isomorphisms. (Hint: same dimsion and nullity is zero)\

Consider the map,

\[ T = \phi_\g^{-1}\cdot \phi_\b: F^n\rightarrow F\]

Let's find the matrix representation of $T$ with respect to the standard basis of $F^n$. The j$^\text{th}$ column is given by $\phi^{-1}_\g\cdot\phi_\b\l( e_j \r) = \phi^{-1}_\g v_j $.

\[ \phi^{-1}_\g v_j = \phi^{-1}_\g \l(a_{1j}v_1^`+\dots+a_{nj}v_n^`\r) \]

\[ \phi^{-1}_\g v_j =  a_{1j}\phi^{-1}_\g v_1^`+\dots+a_{nj}\phi^{-1}_\g v_n^` \]

\[ \phi^{-1}_\g v_j =  a_{1j}e_1+\dots+a_{nj}e_n \]

Entries of the above matrix is precisely the entries of $\l[I_V\r]^\g_\b$.

\subsection{Invertibility}

\begin{definition}[Invertible maps]
	A linear map $T:V\rightarrow W$ is called invertible if there exists $S:W\rightarrow V$ such that $T\cdot S = I_W$ and $S\cdot T = I_V$.
\end{definition}

\begin{remark}
	Due to rank-nullity, if $T:V\rightarrow W$ is invertible then both $V$ and $W$ are of same dimension.
\end{remark}

\begin{definition}[Invertible matrices]
	A matrix $A\in M_{m\times n}\l(\R\r)$ is called invertible if there exists $B\in M_{n\times m}$ such that $AB = I_m$ and $BA=I_n$.
\end{definition}

\begin{remark}
	If $A\in M_{m\times n}\l(\R\r)$ is invertible then $m=n$.
\end{remark}

\begin{theorem}
	Linear map $T:V\rightarrow W$ with $\b$ and $\g$ being ordered basis of $V$ and $W$ is invertible if and only if $\l[T\r]^\g_\b$ is invertible.
\end{theorem}

\begin{proof}
	If $T$ is invertible then dim $V = $ dim $W = n$ and $\l[T\r]_\b^\g\in M_n\l(\R\r)$. As $T\cdot T^{-1} = I_W$ and $T^{-1}\cdot T = I_V$, we get.
	\[ I_n = \l[I_V\r]_\b = \l[T^{-1}\cdot T\r]_\b = \l[T^{-1}\r]^\b_\g \l[T\r]^\g_\b \]
	\[ I_n = \l[I_W\r]_\g = \l[T\cdot T^{-1}\r]_\g = \l[T\r]^\g_\b \l[T^{-1}\r]^\b_\g \]
	This proves, $\l[T\r]^\g_\b$ is invertible if $T$ is invertible.\\
	For converse, assume $A= \l[T\r]^\g_\b$ is invertible with $B$ as its invertible, we know $m=n$. It suffices to show that $T$ is injective. If $v\in N\l(T\r)$, then $Tv=0$. As,
	\[A\l[v\r]^\b=\l[T\r]^\g_\b\l[v\r]^\b = \l[Tv\r]^\g = 0 \]
	Multiplying by $B$ on the left, we conclude that $\l[v\r]^\b = 0$. This implies that $v=0$ and that $T$ is injective.
\end{proof}

\subsection{Similar matrices}

\begin{definition}[Similar matrices]
	We say $A,B\in M_n\l(\R\r)$ are similar if there exists an invertible matrix $Q\in M_n\l(\R\r)$ such that $QAQ^{-1}=B$.
\end{definition}

$Q$ is not unique, as $\lambda Q$ also will satisfy the condition if $Q$ does.

\begin{remark}
	$A\sim B$ if $A$ is similar to $B$. $\sim$ is an equivalence relation on $M_n\l(\R\r)$.
	\begin{itemize} 
		\item \textit{i) [Reflexive]} $I_nAI_n^{-1} = A$.
		\item \textit{ii) [Symmetric]} If $QAQ^{-1} = B$ then $Q^{-1}BQ = A$.
		\item \textit{iii) [Transitive]} If $QAQ^{-1} = B$ and $PBP^{-1}=C$ then, $\l(PQ\r)A\l(PQ\r)^{-1} = PQAQ^{-1}P^{-1}=PBP^{-1}=C$.
	\end{itemize}
			
\end{remark}

\begin{example}[Cross product]
	$a\in \R^3$, $a=\l(x_1,x_2,x_3\r)$ and $||a|| = x_1^2 + x_2^2 + x_3^2 = 1$.
	\[ X_a: \R^3\rightarrow \R^3, v \rightarrow a \times v \]
	\[ a\times \l(v + w\r) = a \times v + a \times w \]
	\[ a \times \l(\lambda v\r) = \lambda a \times v \]
	Therefore $X_a$ is a linear map.
	\[ a^\perp = \{ w \in \R^3 | a\cdot w = 0\} \]
	Range of $X_a = a^\perp$, Null space of $X_a = $span$\l(a\r)$. 
	\[ \b = \{ e_1,e_2,e_3\} \]
	\[ \g = \{a,b,a\times b \]
	Where $b\in a^\perp$
	\[ \l[ X_a \r]_\b = \begin{pmatrix} 0&-x_3&x_2\\x_3&0&-x_1\\-x_2&x_1&0\end{pmatrix} \]
	\[ \l[ X_a \r]_\g = \begin{pmatrix} 0&0&0\\0&0&-1\\0&1&0\end{pmatrix} \]
	\[ \l[ X_a \r]_\g^2 = \begin{pmatrix} 0&0&0\\0&-1&0\\0&-1&0\end{pmatrix} \]
	$\l[ X_a \r]_g$ and $\l[ X_a \r]_b$ are similar matrices where $Q= \l[ I_{\R^3} \r]^\b_\g$.
	\[ Q= \l[ I_{\R^3} \r]^\b_\g = \begin{pmatrix} |&|&|\\a&b&a\times b\\|&|&| \end{pmatrix} \]

\end{example}

\subsection{Trace}

\begin{definition}[Trace]
	For $A\in M_n\l(F\r)$, tr$\l(A\r) = a_{11}+\dots+a_{nn}$. Where,
	\[ A = \begin{pmatrix} a_{11}&a_{12}&\cdots&a_{1n}\\
			       a_{21}&a_{22}&\cdots&a_{2n}\\
			       \vdots&\vdots&\ddots&\vdots\\
  			       a_{n1}&a_{n2}&\cdots&a_{nn}\end{pmatrix} \]
\end{definition}

\begin{definition}[Trace]
	Given a linear map $T:V\rightarrow V$ from a finite dimensional vector space to itself, trace of the linear map is given by tr$\l(T\r)=$ tr$\l(\l[T\r]_\b\r)$ where $\b$ is a basis of $V$.
\end{definition}

\begin{observation}
	Observation: $\text{tr}\l(AB\r) = \text{tr}\l(BA\r)$
\end{observation}

\begin{proof}
	\[ \text{tr}\l(AB\r) = \l(AB\r)_{11}+\dots+\l(AB\r)_{nn} = \sum_{i=1}^n a_{1i}b_{i1}+\dots+\sum^n_{i=1}a_{ni}b_{in} = \sum_{i,j=1}^na_{ij}b_{ij} \]
	\[\sum_{i,j=1}^na_{ij}b_{ij} = \sum_{i,j=1}^nb_{ij}a_{ij} = \text{tr}\l(BA\r) \]
\end{proof}

Therefore, 

\[ 
\text{tr}\l( \l[ T \r]^\g_\g \r) = 
\text{tr}\l( \l[ I_V \r]^\g_\b \l[ T \r]^\b_\b \l[ I_V \r]^\b_\g \r) =
\text{tr}\l( \l( \l[ I_V \r]^\g_\b \l[ T \r]^\b_\b \r) \l[ I_V \r]^\b_\g \r) = 
\text{tr}\l( \l[ I_V \r]^\b_\g \l( \l[ I_V \r]^\g_\b \l[ T \r]^\b_\b \r) \r)  \]
\[ 
\text{tr}\l( \l[ I_V \r]^\b_\g \l( \l[ I_V \r]^\g_\b \l[ T \r]^\b_\b \r) \r) =  \text{tr}\l( \l[ I_V \r]^\b_\g  \l[ I_V \r]^\g_\b \l[ T \r]^\b_\b \r) = \text{tr} \l( \l[ T \r]_b^\b \r)  \]

This proves that the definition of trace of a linear map is well defined.

\begin{example}
	\[ I_V:V\rightarrow V, v\rightarrow v \]
	$\l[ I_V \r] = I_n$ where n is the dimension of $V$, trace = n.
\end{example}



\end{document}
